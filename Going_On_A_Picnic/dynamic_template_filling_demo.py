# dynamic_template_filling_demo.py

import random
import string
import os
from openai import OpenAI  # Import the OpenAI client

# Set your OpenAI API key
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_KEY)

# Define the rule templates for each rule type with placeholders.
rule_templates = {
    "attribute_based": "An object must have the attribute {attribute} equal to {value}.",
    "categorical": "An object must belong to the category '{category}'.",
    "relational": "An object must have a {relation} relationship with another object.",
    "logical": "An object must have {attribute1} equal to {value1} AND {attribute2} equal to {value2}.",
    "semantic": "An object must be used for {purpose}."
}

# Dictionary to store rules under each rule type (simulating separate databases)
rules_storage = {
    "attribute_based": [],
    "categorical": [],
    "relational": [],
    "logical": [],
    "semantic": []
}

# Function to send a prompt to the LLM and get the response
def get_llm_response(prompt, sysprompt="You are a helpful assistant."):
    try:
        response = client.chat.completions.create(
            model="gpt-4",  # Replace with "gpt-3.5-turbo" or your desired model
            messages=[
                {"role": "system", "content": sysprompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )
        generated_text = response.choices[0].message.content.strip()
        return generated_text
    except Exception as e:
        print("Error communicating with OpenAI:\n", e)
        return ""

# Function to generate a rule prompt for each rule type
def generate_rule_prompt(rule_type):
    template = rule_templates[rule_type]
    prompt = f"""Please fill in the placeholders in the following rule template to create a specific rule. The placeholders are enclosed in curly braces {{}}. Do not include any placeholders in your final rule.

Rule Template: '{template}'"""
    return prompt

# Function to collect and process the LLM outputs from the rule prompt
def generate_rule_with_llm(rule_type):
    # Generate the template prompt based on the rule type
    prompt = generate_rule_prompt(rule_type)
    
    # Debugging: Print the prompt to ensure it's correct
    print(f"Generated prompt:\n{prompt}\n")
    
    # Call the LLM with the prompt to generate a specific rule
    llm_response = get_llm_response(prompt)
    
    # Process and return the rule generated by the LLM
    generated_rule = llm_response.strip()
    return generated_rule

# Function to generate and store the rule under the correct rule type
def generate_and_store_rule(rule_type, rule_description):
    # Store the rule in the correct category within rules_storage
    rules_storage[rule_type].append(rule_description)
    
    print(f"Generated rule type: {rule_type}")
    print(f"Rule: {rule_description}\n")

def generate_rule_chatgpt(rngstate, rule_type, domain=None, difficulty=None, init_examples=None):
    # Generate the rule using LLM with dynamic template filling
    rule_description = generate_rule_with_llm(rule_type)
    
    # Now, we need to convert the generated rule into executable code
    # We'll ask the LLM to generate a function that implements the rule
    prompt = f"""Given the following rule:
'{rule_description}'

Write a Python function called 'generated_fn' that takes a single argument x (a string) and returns True if x satisfies the rule, and False otherwise.

Do not include any additional text or explanations. Provide only the function definition."""
    
    ans = get_llm_response(prompt=prompt, sysprompt='You are a helpful assistant that writes code as per instructions.')
    
    # Process the LLM's response to extract the function code
    ans_strip = ''
    for line in ans.splitlines():
        if not line.startswith('```'):
            ans_strip += line + '\n'
    
    # Execute the generated function code safely
    try:
        local_namespace = {}
        exec(ans_strip, globals(), local_namespace)
        if 'generated_fn' in local_namespace:
            generated_fn = local_namespace['generated_fn']
        else:
            raise Exception('Problem assigning "generated_fn"')
    except Exception as e:
        print("Error in Generated Code:", e)
        return rngstate, None, rule_description

    return rngstate, generated_fn, rule_description

def word_generator(rngstate, k):
    random.setstate(rngstate)
    word = ''.join(random.choice(string.ascii_lowercase) for _ in range(k))
    return random.getstate(), word

def make_word_generator(k):
    return lambda rngstate: word_generator(rngstate, k)

def generate_rule_game(rngstate, domain=None, difficulty=None, init_examples=None):
    # Randomly select a rule type
    rule_type = random.choice(list(rule_templates.keys()))
    rngstate, rule, rule_description = generate_rule_chatgpt(rngstate, rule_type, domain, difficulty, init_examples)
    wordgen = make_word_generator(4)
    return [rngstate, rule, wordgen, rule_type, rule_description]

def generate_more_examples(rule_game_instance):
    rngstate, rule, word_generator = rule_game_instance[:3]
    rngstate, random_word = word_generator(rngstate)
    if rule is not None:
        is_rule_true = rule(random_word)
    else:
        is_rule_true = None
    return [rngstate, random_word, is_rule_true]

# Main function focusing on Dynamic Template Filling and storing the LLM-generated rule
if __name__ == "__main__":
    random.seed(42)
    print('Demonstration of Dynamic Template Filling via LLM\n')

    # Generate a rule game using the LLM
    rngstate = random.getstate()
    rngstate, rule, wordgen, rule_type, rule_description = generate_rule_game(rngstate)

    # Display the generated rule and rule type
    print(f"Generated rule type: {rule_type}")
    print(f"Generated rule description: {rule_description}\n")

    # Generate some examples using the generated rule
    if rule is not None:
        print("Generated examples:")
        for _ in range(5):
            rngstate, random_word, is_rule_true = generate_more_examples((rngstate, rule, wordgen))
            print(f"{random_word}: {is_rule_true}")
    else:
        print("Could not generate rule function.")

    # Store the rule in rules_storage
    generate_and_store_rule(rule_type, rule_description)

    # Display the contents of the rules storage to verify the rule is stored properly
    print("\nCurrent rules storage:")
    for rt, rules in rules_storage.items():
        print(f"\nRule Type: {rt}")
        for idx, r in enumerate(rules):
            print(f"{idx + 1}. {r}")