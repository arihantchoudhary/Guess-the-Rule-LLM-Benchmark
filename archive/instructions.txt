# Project Instructions

## Overview
Our project focuses on developing a benchmark for evaluating the abstract reasoning capabilities of Large Language Models (LLMs) through "Guess the Rule" games. This benchmark is designed to be dynamic, reusable, and robust against memorization, providing a more accurate assessment of LLMs' reasoning abilities.

## Project Structure
The benchmark consists of three main components:
1. **Dynamic Dataset Creation**: Continuously generates unique game instances with accurate ground truths across various domains and difficulty levels.
2. **Benchmark Framework**: Provides evaluation scores for model outputs on generated game datasets and includes a survey of current LLMs' performance.
3. **Interactive Game Interface**: A user-friendly interface for researchers to engage with the games, including downloadable resources and support channels.

## Tasks and Assignments
- **Dynamic Games with LLM**: 
  - Juno Lee: Work on syntax-style games (static-dynamic hybrid) and adapt code into Michael's GitHub.
  - Michael Lu: Create a writeup summary and demo covering rule type definition, dynamic template filling, and LLM response handling.
  - Xiang Zheng: Focus on dynamic game generation and dataset verification using LLM-boosted, natural language-generated datasets.
  - Ali Shazal and Arihant Choudhary: Develop static games and create instances of the picnic game.

- **Gamestate Development**: Implement a system to play games with a client LLM, verifying answers and providing feedback as 'correct' or 'incorrect'.

## Action Items (Due Wednesday 6 PM)
- Complete the assigned tasks for dynamic and static game development.
- Ensure integration of code into the designated GitHub repository.
- Prepare a summary and demo of the project components.

## Research Goals
- Develop a perpetually reusable benchmark for LLMs.
- Publish development, validation, and evaluation results.
- Facilitate effective dissemination within the scientific community.

## References
- Existing benchmarks like GSM8k, MATH, and HumanEval.
- Challenges in evaluating abstract reasoning due to dataset contamination and overfitting.
- Proposed solutions and the need for dynamic benchmarking frameworks.

## Team Members
- Michael Lu
- Ali Shazal
- Juno Lee
- Xiang Zheng
- Arihant Choudhary

## Institution
- University of California, Berkeley

## Date
- September 30, 2024